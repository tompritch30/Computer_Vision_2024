{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tompritch30/Computer_Vision_2024/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MBmOdXoenFw"
      },
      "source": [
        "# Lab 4: Simple Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3iS23ADfo4w"
      },
      "source": [
        "## Version history\n",
        "\n",
        "| Date | Author | Description |\n",
        "|:----:|:------:|:------------|\n",
        "2021-02-03 | Josiah Wang | First version |\n",
        "2021-11-13 | Josiah Wang | Optimisation with derivatives: $x$ should be $x^{(i)}$ |\n",
        "2022-11-02 | Josiah Wang | Updated to `ax = fig.add_subplot(projection='3d')`. The original `fig.gca(projection='3d')` is deprecated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EbbkgqOgZK_"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab exercise is for you to gain some experience implementing and training a simple linear regression model from scratch. This will help you improve your understanding of linear regression and machine learning optimisation.\n",
        "\n",
        "By the end of this lab exercise, you will have\n",
        "- implemented a simple linear regression model\n",
        "- defined and implemented a loss function\n",
        "- optimised the parameters of your model using gradient descent\n",
        "\n",
        "There will be a bit less coding required on your side in this exercise compared to previous exercises. The aim is for you to try to really understand linear regression at implementation level to complement the lectures, which will help you in future weeks as you move on to Neural Networks in your coursework assignments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0iaMjnIWEpe"
      },
      "source": [
        "## Simple Linear Regression\n",
        "\n",
        "In this tutorial, we will focus on the **regression task**. For simplicity, we will implement a *simple linear regression* model with one input variable and one output variable. More specifically, our task is to predict the value of $y$ given the input $x$.\n",
        "\n",
        "Let us develop our simple linear regressor with a simple toy example to make sure that our model works correctly. You can later apply it to a bigger dataset if desired."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w2N6AZsYJUd",
        "outputId": "3c35b630-f112-4764-e1ed-785705d650c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# toy dataset\n",
        "x_train = np.array([1.0, 1.2, 2.0, 3.5, 4.0, 5.0])\n",
        "y_train = np.array([3.1, 3.5, 5.0, 7.9, 9.1, 10.9])\n",
        "x_test = np.array([2.5, 3.0, 4.5])\n",
        "y_test = np.array([6.0, 7.0, 10.1])\n",
        "\n",
        "# plot toy data\n",
        "plt.scatter(x_train, y_train, c=\"blue\")\n",
        "plt.scatter(x_test, y_test, c=\"red\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGwCAYAAABcnuQpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlPklEQVR4nO3dfXBU1f3H8c+ymECVXcUC2ZCFIEKoKIKlZYLmByjKIHWCGawPaKkPrWPTMamtLczUp/oQtI6Gtg5SbYFiqdW4Mq1VEdRAFLQ8xUbHUqBRQ1il0+puoLq1m/P7YycpSx7Ihs3eezbv18ydZM89C9/jcdyP555712OMMQIAALDUAKcLAAAAOB6EGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqw10uoC+1traqgMHDmjIkCHyeDxOlwMAAHrAGKOWlhbl5+drwIDu116yPswcOHBAwWDQ6TIAAEAvNDU1qaCgoNs+WR9mhgwZIinxD8Pn8zlcDQAA6IloNKpgMNj+Od6drA8zbZeWfD4fYQYAAMv0ZIsIG4ABAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKzmaJjZvHmzLrnkEuXn58vj8WjdunVJ50OhkC666CKdeuqp8ng8qq+vd6ROAADgXo6GmcOHD+vss8/WI4880uX58847T/fff3+GKwMAALZw9KF5c+fO1dy5c7s8f80110iS3nvvvQxVBAAAbJN1TwCOxWKKxWLtr6PRqIPVAACQneJxqa5OCoelQEAqKZG8XmdqyboNwFVVVfL7/e0HXzIJAEB6hUJSYaE0a5Z01VWJn4WFiXYnZF2YWbJkiSKRSPvR1NTkdEkAAGSNUEhasEDavz+5vbk50e5EoMm6MJObm9v+pZJ8uSQAAOkTj0sVFZIxHc+1tVVWJvplUtaFGQAA0Dfq6jquyBzJGKmpKdEvkxzdAHzo0CHt3bu3/XVjY6Pq6+s1dOhQjRo1Sv/617/0wQcf6MCBA5Kk3bt3S5Ly8vKUl5fnSM0AAPRX4XB6+6WLoysz27dv15QpUzRlyhRJ0i233KIpU6bo9ttvlyT94Q9/0JQpUzRv3jxJ0hVXXKEpU6bo0UcfdaxmAAD6q0Agvf3SxWNMZ1e+skc0GpXf71ckEmH/DAAAxyEeT9y11Nzc+b4Zj0cqKJAaG4//Nu1UPr/ZMwMAAHrE65WWLUv87vEkn2t7XV2d+efNEGYAAECPlZVJNTXSyJHJ7QUFifaysszXlHVPAAYAAH2rrEwqLXXPE4AJMwAAIGVerzRzptNVJHCZCQAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGoDnS4AAICsFI9LdXVSOCwFAlJJieT1Ol1VViLMAACQbqGQVFEh7d//v7aCAmnZMqmszLm6shSXmQAASKdQSFqwIDnISFJzc6I9FHKmrixGmAEAIF3i8cSKjDEdz7W1VVYm+iFtCDMAAKRLXV3HFZkjGSM1NSX6IW0IMwAApEs4nN5+6BFHw8zmzZt1ySWXKD8/Xx6PR+vWrUs6b4zR7bffrkAgoMGDB2v27Nnas2ePM8UCAHAsgUB6+6FHHA0zhw8f1tlnn61HHnmk0/MPPPCAfvazn+nRRx/Vm2++qRNPPFFz5szRZ599luFKAQDogZKSxF1LHk/n5z0eKRhM9EPaOHpr9ty5czV37txOzxljVF1drR//+McqLS2VJP3mN7/RiBEjtG7dOl1xxRWZLBUAgGPzehO3Xy9YkAguR24Ebgs41dU8bybNXLtnprGxUR9++KFmz57d3ub3+zVt2jRt3bq1y/fFYjFFo9GkAwCAjCkrk2pqpJEjk9sLChLtPGcm7Vz70LwPP/xQkjRixIik9hEjRrSf60xVVZXuuuuuPq0NAIBulZVJpaU8AThDXBtmemvJkiW65ZZb2l9Ho1EFg0EHKwIA9EterzRzptNV9AuuvcyUl5cnSfroo4+S2j/66KP2c53Jzc2Vz+dLOgAAQPZybZgZM2aM8vLy9PLLL7e3RaNRvfnmmyouLnawMgAA4CaOXmY6dOiQ9u7d2/66sbFR9fX1Gjp0qEaNGqXKykrdc889GjdunMaMGaPbbrtN+fn5mj9/vnNFAwAAV3E0zGzfvl2zZs1qf92212XRokVatWqVfvjDH+rw4cP69re/rU8++UTnnXeeXnzxRQ0aNMipkgEAgMt4jOns27CyRzQald/vVyQSYf8MAACWSOXz27V7ZgAAAHqCMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKw20OkCAAD9Uzwu1dVJ4bAUCEglJZLX63RVsBFhBgCQcaGQVFEh7d//v7aCAmnZMqmszLm6YCcuMwEAMioUkhYsSA4yktTcnGgPhZypC/YizAAAMiYeT6zIGNPxXFtbZWWiH9BThBkAQMbU1XVckTmSMVJTU6If0FOEGQBAxoTD6e0HSBaEmZaWFlVWVmr06NEaPHiwpk+frm3btjldFgCgFwKB9PYDJAvCzA033KANGzZozZo1amho0EUXXaTZs2erubnZ6dIAACkqKUncteTxdH7e45GCwUQ/oKdcHWY+/fRTPfPMM3rggQf0f//3fzr99NN155136vTTT9fy5cudLg8AkCKvN3H7tdQx0LS9rq7meTNIjavDzH//+1/F43ENGjQoqX3w4MF67bXXOn1PLBZTNBpNOgAA7lFWJtXUSCNHJrcXFCTaec4MUuXqMDNkyBAVFxfr7rvv1oEDBxSPx/XEE09o69atCnexO6yqqkp+v7/9CAaDGa4aAHAsZWXSe+9Jr74qrV2b+NnYSJBB73iM6exuf/fYt2+frrvuOm3evFler1fnnHOOxo8frx07dujdd9/t0D8WiykWi7W/jkajCgaDikQi8vl8mSwdAAD0UjQald/v79Hnt+u/zmDs2LHatGmTDh8+rGg0qkAgoMsvv1ynnXZap/1zc3OVm5ub4SoBAIBTXH2Z6UgnnniiAoGAPv74Y61fv16lpaVOlwQAAFzA9Ssz69evlzFGRUVF2rt3r2699VZNmDBB1157rdOlAQAAF3D9ykwkElF5ebkmTJigb3zjGzrvvPO0fv16nXDCCU6XBgAAXMD1G4CPVyobiAAAgDuk8vnt+pUZAACA7hBmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWc3WYicfjuu222zRmzBgNHjxYY8eO1d133y1jjNOlAQAAlxjodAHduf/++7V8+XKtXr1aEydO1Pbt23XttdfK7/fr5ptvdro8AADgAq4OM1u2bFFpaanmzZsnSSosLNTvfvc7/fnPf3a4MgAA4Bauvsw0ffp0vfzyy/rb3/4mSXrrrbf02muvae7cuV2+JxaLKRqNJh0AYJt4XKqtlX73u8TPeNzpigD3cvXKzOLFixWNRjVhwgR5vV7F43Hde++9WrhwYZfvqaqq0l133ZXBKgEgvUIhqaJC2r//f20FBdKyZVJZmXN1AW7l6pWZp556Sr/97W+1du1a7dy5U6tXr9aDDz6o1atXd/meJUuWKBKJtB9NTU0ZrBgAjk8oJC1YkBxkJKm5OdEeCjlTF+BmHuPiW4OCwaAWL16s8vLy9rZ77rlHTzzxhP7617/26M+IRqPy+/2KRCLy+Xx9VSoAHLd4XCos7Bhk2ng8iRWaxkbJ681oaUDGpfL57eqVmX//+98aMCC5RK/Xq9bWVocqAoC+U1fXdZCRJGOkpqZEPwD/4+o9M5dcconuvfdejRo1ShMnTtSuXbv00EMP6brrrnO6NABIu3A4vf2A/sLVYebnP/+5brvtNn3nO9/RwYMHlZ+frxtvvFG3336706UBQNoFAuntB/QXrt4zkw7smQFgi7Y9M83NiUtKR2PPDPqTrNkzAwD9idebuP1aSgSXI7W9rq4myABHI8wAgIuUlUk1NdLIkcntBQWJdp4zA3Tk6j0zANAflZVJpaWJu5bC4cQemZISVmSArhBmAMCFvF5p5kynqwDswGUmAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYbaDTBQBAr8TjUl2dFA5LgYBUUiJ5vU5XBcABrl+ZKSwslMfj6XCUl5c7XRoAp4RCUmGhNGuWdNVViZ+FhYl2AP2O68PMtm3bFA6H248NGzZIki677DKHKwPgiFBIWrBA2r8/ub25OdFOoAH6nZTDzKJFi7R58+a+qKVTw4YNU15eXvvx3HPPaezYsZoxY0bGagDgEvG4VFEhGdPxXFtbZWWiH4B+I+UwE4lENHv2bI0bN0733Xefmpub+6KuTv3nP//RE088oeuuu04ej6fTPrFYTNFoNOkAkCXq6jquyBzJGKmpKdEPQL+RcphZt26dmpubddNNN+n3v/+9CgsLNXfuXNXU1Ojzzz/vixqT/u5PPvlE3/zmN7vsU1VVJb/f334Eg8E+rQlABoXD6e0HICt4jOlsvbbndu7cqZUrV+rxxx/XSSedpKuvvlrf+c53NG7cuHTV2G7OnDnKycnRH//4xy77xGIxxWKx9tfRaFTBYFCRSEQ+ny/tNQHIoNraxGbfY3n1VWnmzL6uBkAfikaj8vv9Pfr8Pq4NwG0bcjds2CCv16uLL75YDQ0NOuOMM/Twww8fzx/dwfvvv6+NGzfqhhtu6LZfbm6ufD5f0gEgS5SUSAUFUheXmeXxSMFgoh+AfiPlMPP555/rmWee0de+9jWNHj1aTz/9tCorK3XgwAGtXr1aGzdu1FNPPaWf/OQnaS105cqVGj58uObNm5fWPxeARbxeadmyxO9HB5q219XVPG8G6GdSfmheIBBQa2urrrzySv35z3/W5MmTO/SZNWuWTj755DSUl9Da2qqVK1dq0aJFGjiQ5/wB/VpZmVRTk7ir6cjNwAUFiSBTVuZYaQCckfKemTVr1uiyyy7ToEGD+qqmDl566SXNmTNHu3fv1vjx41N6byrX3ABYhCcAA1ktlc/v494A7HaEGQAA7JOxDcAAAABOI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFhtoNMFAOgj8bhUVyeFw1IgIJWUSF6v01UBQNq5fmWmublZV199tU499VQNHjxYZ511lrZv3+50WYC7hUJSYaE0a5Z01VWJn4WFiXYAyDKuXpn5+OOPde6552rWrFl64YUXNGzYMO3Zs0ennHKK06UB7hUKSQsWSMYktzc3J9praqSyMmdqA4A+4DHm6P/iucfixYv1+uuvq66urtd/RjQald/vVyQSkc/nS2N1gAvF44kVmP37Oz/v8UgFBVJjI5ecALhaKp/frr7M9Ic//EFTp07VZZddpuHDh2vKlCl67LHHun1PLBZTNBpNOoB+o66u6yAjJVZrmpoS/QAgS7g6zPz973/X8uXLNW7cOK1fv1433XSTbr75Zq1evbrL91RVVcnv97cfwWAwgxUDDguH09sPACzg6stMOTk5mjp1qrZs2dLedvPNN2vbtm3aunVrp++JxWKKxWLtr6PRqILBIJeZ0D/U1iY2+x7Lq69KM2f2dTUA0GtZc5kpEAjojDPOSGr70pe+pA8++KDL9+Tm5srn8yUdQL9RUpLYE+PxdH7e45GCwUQ/AMgSrg4z5557rnbv3p3U9re//U2jR492qCLA5bxeadmyxO9HB5q219XVbP4FkFVcHWa+973v6Y033tB9992nvXv3au3atfrlL3+p8vJyp0sD3KusLHH79ciRye0FBdyWDSAruXrPjCQ999xzWrJkifbs2aMxY8bolltu0be+9a0ev59bs9Fv8QRgABZL5fPb9WHmeBFmAACwT9ZsAAYAADgWwgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUGOl0A4IR4XKqrk8JhKRCQSkokr9fpqgAAveH6lZk777xTHo8n6ZgwYYLTZcFioZBUWCjNmiVddVXiZ2Fhoh0AYB8rVmYmTpyojRs3tr8eONCKsuFCoZC0YIFkTHJ7c3OivaZGKitzpjYAQO9YkQoGDhyovLw8p8uA5eJxqaKiY5CREm0ej1RZKZWWcskJAGzi+stMkrRnzx7l5+frtNNO08KFC/XBBx902TcWiykajSYdgJTYI7N/f9fnjZGamhL9AAD2cH2YmTZtmlatWqUXX3xRy5cvV2Njo0pKStTS0tJp/6qqKvn9/vYjGAxmuGK4VTic3n4AAHfwGNPZort7ffLJJxo9erQeeughXX/99R3Ox2IxxWKx9tfRaFTBYFCRSEQ+ny+TpcJlamsTm32P5dVXpZkz+7oaAEB3otGo/H5/jz6/rdgzc6STTz5Z48eP1969ezs9n5ubq9zc3AxXBRuUlEgFBYnNvp1FeI8ncb6kJPO1AQB6z/WXmY526NAh7du3T4FAwOlSYBmvV1q2LPG7x5N8ru11dTWbfwHANq4PMz/4wQ+0adMmvffee9qyZYsuvfRSeb1eXXnllU6XBguVlSVuvx45Mrm9oIDbsgHAVq6/zLR//35deeWV+uc//6lhw4bpvPPO0xtvvKFhw4Y5XRosVVaWuP2aJwADQHawbgNwqlLZQAQAANwhlc9v119mAgAA6A5hBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAABgNcIMAACwmlVhZunSpfJ4PKqsrHS6FAAA4BLWhJlt27ZpxYoVmjRpktOlAAAAF7EizBw6dEgLFy7UY489plNOOaXbvrFYTNFoNOkAAADZy4owU15ernnz5mn27NnH7FtVVSW/399+BIPBDFQIAACc4vow8+STT2rnzp2qqqrqUf8lS5YoEom0H01NTX1cIQAAcNJApwvoTlNTkyoqKrRhwwYNGjSoR+/Jzc1Vbm5uH1cGAADcwmOMMU4X0ZV169bp0ksvldfrbW+Lx+PyeDwaMGCAYrFY0rnORKNR+f1+RSIR+Xy+vi4ZAACkQSqf365embngggvU0NCQ1HbttddqwoQJ+tGPfnTMIAMAALKfq8PMkCFDdOaZZya1nXjiiTr11FM7tAMAgP7J9RuAAQAAuuPqlZnO1NbWOl0CAABwEVZmAACA1QgzAADAaoQZAABgNcIMAACwGmEGAABYjTADAACsRpgBAABWI8wAAACrEWYAAIDVrHsCcLaLx6W6OikclgIBqaRE4vs0AQDoGmHGRUIhqaJC2r//f20FBdKyZVJZmXN1AQDgZlxmcolQSFqwIDnISFJzc6I9FHKmLgAA3I4w4wLxeGJFxpiO59raKisT/QAAQDLCjAvU1XVckTmSMVJTU6IfAABIRphxgXA4vf0AAOhPCDMuEAiktx8AAP0JYcYFSkoSdy15PJ2f93ikYDDRDwAAJCPMuIDXm7j9WuoYaNpeV1fzvBkAADpDmHGJsjKppkYaOTK5vaAg0c5zZgAA6BwPzXORsjKptJQnAAMAkArCjMt4vdLMmU5XAQCAPbjMBAAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAatzN1EvxOLdQAwDgBoSZXgiFpIqK5G+6LihIPMWXh9sBAJBZXGZKUSgkLViQHGQkqbk50R4KOVMXAAD9FWEmBfF4YkXGmI7n2toqKxP9AABAZhBmUlBX13FF5kjGSE1NiX4AACAzCDMpCIfT2w8AABw/14eZ5cuXa9KkSfL5fPL5fCouLtYLL7zgSC2BQHr7AQCA4+f6MFNQUKClS5dqx44d2r59u84//3yVlpbqnXfeyXgtJSWJu5Y8ns7PezxSMJjoBwAAMsNjTGfbWd1t6NCh+ulPf6rrr7++w7lYLKZYLNb+OhqNKhgMKhKJyOfzHfff3XY3k5S8Ebgt4NTUcHs2AADHKxqNyu/39+jz2/UrM0eKx+N68skndfjwYRUXF3fap6qqSn6/v/0IBoNpraGsLBFYRo5Mbi8oIMgAAOAEK1ZmGhoaVFxcrM8++0wnnXSS1q5dq4svvrjTvn29MtOGJwADANB3UlmZseIJwEVFRaqvr1ckElFNTY0WLVqkTZs26YwzzujQNzc3V7m5uX1ek9crzZzZ538NAAA4BitWZo42e/ZsjR07VitWrDhm31SSHQAAcIes3TPTprW1NelSEgAA6L9cf5lpyZIlmjt3rkaNGqWWlhatXbtWtbW1Wr9+vdOlAQAAF3B9mDl48KC+8Y1vKBwOy+/3a9KkSVq/fr0uvPBCp0sDAAAu4Pow86tf/crpEgAAgItZuWcGAACgDWEGAABYjTADAACsRpgBAABWc/0G4OPV9kzAaDTqcCUAAKCn2j63e/Js36wPMy0tLZKU9i+cBAAAfa+lpUV+v7/bPlZ+nUEqWltbdeDAAQ0ZMkQejyetf3bbl1g2NTVl5VclMD77ZfsYGZ/9sn2MjK/3jDFqaWlRfn6+BgzofldM1q/MDBgwQAUFBX36d/h8vqz8l7QN47Nfto+R8dkv28fI+HrnWCsybdgADAAArEaYAQAAViPMHIfc3Fzdcccdys3NdbqUPsH47JftY2R89sv2MTK+zMj6DcAAACC7sTIDAACsRpgBAABWI8wAAACrEWYAAIDVCDNd2Lx5sy655BLl5+fL4/Fo3bp1x3xPbW2tzjnnHOXm5ur000/XqlWr+rzO3kp1fLW1tfJ4PB2ODz/8MDMFp6iqqkpf+cpXNGTIEA0fPlzz58/X7t27j/m+p59+WhMmTNCgQYN01lln6fnnn89Atb3TmzGuWrWqwxwOGjQoQxWnZvny5Zo0aVL7w7iKi4v1wgsvdPsem+ZPSn2MNs1fZ5YuXSqPx6PKyspu+9k2j216Mj7b5vDOO+/sUO+ECRO6fY8T80eY6cLhw4d19tln65FHHulR/8bGRs2bN0+zZs1SfX29KisrdcMNN2j9+vV9XGnvpDq+Nrt371Y4HG4/hg8f3kcVHp9NmzapvLxcb7zxhjZs2KDPP/9cF110kQ4fPtzle7Zs2aIrr7xS119/vXbt2qX58+dr/vz5evvttzNYec/1ZoxS4kmdR87h+++/n6GKU1NQUKClS5dqx44d2r59u84//3yVlpbqnXfe6bS/bfMnpT5GyZ75O9q2bdu0YsUKTZo0qdt+Ns6j1PPxSfbN4cSJE5Pqfe2117rs69j8GRyTJPPss8922+eHP/yhmThxYlLb5ZdfbubMmdOHlaVHT8b36quvGknm448/zkhN6Xbw4EEjyWzatKnLPl//+tfNvHnzktqmTZtmbrzxxr4uLy16MsaVK1cav9+fuaLS7JRTTjGPP/54p+dsn7823Y3R1vlraWkx48aNMxs2bDAzZswwFRUVXfa1cR5TGZ9tc3jHHXeYs88+u8f9nZo/VmbSZOvWrZo9e3ZS25w5c7R161aHKuobkydPViAQ0IUXXqjXX3/d6XJ6LBKJSJKGDh3aZR/b57AnY5SkQ4cOafTo0QoGg8dcBXCLeDyuJ598UocPH1ZxcXGnfWyfv56MUbJz/srLyzVv3rwO89MZG+cxlfFJ9s3hnj17lJ+fr9NOO00LFy7UBx980GVfp+Yv679oMlM+/PBDjRgxIqltxIgRikaj+vTTTzV48GCHKkuPQCCgRx99VFOnTlUsFtPjjz+umTNn6s0339Q555zjdHndam1tVWVlpc4991ydeeaZXfbrag7dui/oSD0dY1FRkX79619r0qRJikQievDBBzV9+nS98847ff6FrL3R0NCg4uJiffbZZzrppJP07LPP6owzzui0r63zl8oYbZs/SXryySe1c+dObdu2rUf9bZvHVMdn2xxOmzZNq1atUlFRkcLhsO666y6VlJTo7bff1pAhQzr0d2r+CDPokaKiIhUVFbW/nj59uvbt26eHH35Ya9ascbCyYysvL9fbb7/d7XVe2/V0jMXFxUn/1z99+nR96Utf0ooVK3T33Xf3dZkpKyoqUn19vSKRiGpqarRo0SJt2rSpyw97G6UyRtvmr6mpSRUVFdqwYYOrN7n2Vm/GZ9sczp07t/33SZMmadq0aRo9erSeeuopXX/99Q5WlowwkyZ5eXn66KOPkto++ugj+Xw+61dluvLVr37V9QHhu9/9rp577jlt3rz5mP/X09Uc5uXl9WWJxy2VMR7thBNO0JQpU7R3794+qu745OTk6PTTT5ckffnLX9a2bdu0bNkyrVixokNfW+cvlTEeze3zt2PHDh08eDBp9TYej2vz5s36xS9+oVgsJq/Xm/Qem+axN+M7mtvn8Ggnn3yyxo8f32W9Ts0fe2bSpLi4WC+//HJS24YNG7q99m27+vp6BQIBp8volDFG3/3ud/Xss8/qlVde0ZgxY475HtvmsDdjPFo8HldDQ4Nr5/Fora2tisVinZ6zbf660t0Yj+b2+bvgggvU0NCg+vr69mPq1KlauHCh6uvrO/2gt2keezO+o7l9Do926NAh7du3r8t6HZu/Pt1ebLGWlhaza9cus2vXLiPJPPTQQ2bXrl3m/fffN8YYs3jxYnPNNde09//73/9uvvCFL5hbb73VvPvuu+aRRx4xXq/XvPjii04NoVupju/hhx8269atM3v27DENDQ2moqLCDBgwwGzcuNGpIXTrpptuMn6/39TW1ppwONx+/Pvf/27vc80115jFixe3v3799dfNwIEDzYMPPmjeffddc8cdd5gTTjjBNDQ0ODGEY+rNGO+66y6zfv16s2/fPrNjxw5zxRVXmEGDBpl33nnHiSF0a/HixWbTpk2msbHR/OUvfzGLFy82Ho/HvPTSS8YY++fPmNTHaNP8deXou32yYR6PdKzx2TaH3//+901tba1pbGw0r7/+upk9e7b54he/aA4ePGiMcc/8EWa60HYr8tHHokWLjDHGLFq0yMyYMaPDeyZPnmxycnLMaaedZlauXJnxunsq1fHdf//9ZuzYsWbQoEFm6NChZubMmeaVV15xpvge6GxskpLmZMaMGe3jbfPUU0+Z8ePHm5ycHDNx4kTzpz/9KbOFp6A3Y6ysrDSjRo0yOTk5ZsSIEebiiy82O3fuzHzxPXDdddeZ0aNHm5ycHDNs2DBzwQUXtH/IG2P//BmT+hhtmr+uHP1hnw3zeKRjjc+2Obz88stNIBAwOTk5ZuTIkebyyy83e/fubT/vlvnzGGNM3679AAAA9B32zAAAAKsRZgAAgNUIMwAAwGqEGQAAYDXCDAAAsBphBgAAWI0wAwAArEaYAQAAViPMAAAAqxFmAACA1QgzAADAaoQZAFb5xz/+oby8PN13333tbVu2bFFOTo5efvllBysD4BS+aBKAdZ5//nnNnz9fW7ZsUVFRkSZPnqzS0lI99NBDTpcGwAGEGQBWKi8v18aNGzV16lQ1NDRo27Ztys3NdbosAA4gzACw0qeffqozzzxTTU1N2rFjh8466yynSwLgEPbMALDSvn37dODAAbW2tuq9995zuhwADmJlBoB1/vOf/+irX/2qJk+erKKiIlVXV6uhoUHDhw93ujQADiDMALDOrbfeqpqaGr311ls66aSTNGPGDPn9fj333HNOlwbAAVxmAmCV2tpaVVdXa82aNfL5fBowYIDWrFmjuro6LV++3OnyADiAlRkAAGA1VmYAAIDVCDMAAMBqhBkAAGA1wgwAALAaYQYAAFiNMAMAAKxGmAEAAFYjzAAAAKsRZgAAgNUIMwAAwGqEGQAAYLX/B29JXxjO5LhUAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YagIkbxvaC-b"
      },
      "source": [
        "### Model\n",
        "\n",
        "As you should be able to see from the plot, the toy dataset can almost be perfectly modelled with a straight line. The model should also be able to pretty accurately predict the value of $y$ of the test set.\n",
        "\n",
        "Assuming you still remember your high school Maths, a straight line is represented as $y = wx + b$, where $w$ is the slope and $b$ the intercept/bias. Our objective is to find the line that best fits our training data. More specifically, we want our regressor to automatically learn the parameters $w$ and $b$ such that we can accurately predict the real-valued label ${\\hat y}$ given an example $x$. The objective is to get ${\\hat y}$ to be as close as possible to the values of $y$ of the training data (and presumably the true $y$).\n",
        "\n",
        "Let us now build our simple linear regression model. Complete the `forward()` method of the `SimpleLinearRegression` class below to return the value of the output $y$ given an input `x` and the current weight `w` and bias `b` of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLNpOSjXUQrY"
      },
      "source": [
        "from numpy.random import default_rng\n",
        "\n",
        "class SimpleLinearRegression:\n",
        "    def __init__(self, random_generator=default_rng()):\n",
        "        # initialise the slope with a random value drawn from a standard normal\n",
        "        # distribution (mean=0, stddev=1)\n",
        "        self.w = random_generator.standard_normal()\n",
        "\n",
        "        # initialise bias to 0\n",
        "        self.b = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Perform forward pass given an input x\n",
        "\n",
        "        Args:\n",
        "            x (float): input instance\n",
        "\n",
        "        Returns:\n",
        "            float: the output of the model given the current weights\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Complete this\n",
        "        return ????\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        \"\"\" Placeholder for later\"\"\"\n",
        "        pass\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        \"\"\" Placeholder for later\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "## Quick test: This should return 8\n",
        "model = SimpleLinearRegression()\n",
        "model.w = 3\n",
        "model.b = 2\n",
        "x = 2\n",
        "y_hat = model.forward(x)\n",
        "print(y_hat) # should print 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9mtsqFXBNNa"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "From the plot and the numbers, you may have manually worked out that the 'best' line would be $\\hat{y} \\approx 2x+1$, that is, the optimal parameter values are $w \\approx 2$ and $b \\approx 1$.\n",
        "\n",
        "What constitutes the 'best' line? We will first have to define what 'best' actually means. Intuitively, the 'best' line would be the one that goes through all training points as closely as possible.\n",
        "\n",
        "To enable our model to automatically learn what the parameter values of the 'best' line are from training examples, we will have to formally define that we mean by 'best'. We define this via a *loss function* (or cost function). For this tutorial, we will use the loss function as in the lectures - the **sum of squared differences** between the predicted label vs. the ground truth label across the training instances.\n",
        "\n",
        "$$L = \\frac{1}{2} \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 = \\frac{1}{2} \\sum_{i=1}^{N} \\left(wx^{(i)} + b - y^{(i)}\\right)^2$$\n",
        "\n",
        "Our objective is to select the parameters $\\theta = \\{ w, b \\}$ that **minimise** the loss (or error).\n",
        "\n",
        "$$\\theta = argmin_{\\theta} \\, L$$\n",
        "\n",
        "To make things easy, let us implement the loss function for a **single** instance $x$:\n",
        "\n",
        "$$L^{(i)} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$$\n",
        "\n",
        "Complete the `loss()` method of `SimpleLinearRegression` below to return the individual loss for an instance `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZM0MAgiE2UZ"
      },
      "source": [
        "# Loss method for SimpleLinearRegression\n",
        "def loss(self, x, y):\n",
        "    \"\"\" Compute the loss for an input x\n",
        "\n",
        "    Args:\n",
        "        x (float): input instance\n",
        "        y (float): ground truth output\n",
        "\n",
        "    Returns:\n",
        "        float: the model loss for an instance x\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Complete this\n",
        "\n",
        "    return ????\n",
        "\n",
        "\n",
        "# A quick hack to bind this function as the SimpleLinearRegression.loss() method\n",
        "SimpleLinearRegression.loss = loss\n",
        "\n",
        "\n",
        "## Quick test: This should return 0.25\n",
        "model = SimpleLinearRegression()\n",
        "model.w = 3\n",
        "model.b = 2\n",
        "x = 2.0\n",
        "y = 8.5\n",
        "test_loss = model.loss(x, y)\n",
        "print(test_loss) # should print 0.25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxwg9EPiGu2l"
      },
      "source": [
        "### Optimisation by brute force search\n",
        "\n",
        "Now, how do we get the model to automatically figure out the optimal parameters from training data? Remember that the optimal parameter values are the ones that minimise the loss function. A naive approach would be to compute the loss for different combinations of $w$ and $b$ and selecting the combination that results in the smallest loss.\n",
        "\n",
        "The code below will search for $w$ between $0$ and $4$, and for $b$ between $0$ and $2$ to find the best combination of $w$ and $b$. Examine the code, and try to understand what it is doing, then run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyV8OCZSIdrj"
      },
      "source": [
        "model = SimpleLinearRegression()\n",
        "\n",
        "# to store all losses for later use\n",
        "losses = []\n",
        "\n",
        "# the parameters to search\n",
        "weights = np.arange(0, 4.1, 0.2)\n",
        "biases = np.arange(0, 2.1, 0.2)\n",
        "\n",
        "# for storing the loss in a matrix for visualisation later\n",
        "loss_matrix = np.zeros((len(weights), len(biases)))\n",
        "\n",
        "# compute loss for each (w,b) combination\n",
        "for i, w in enumerate(weights):\n",
        "    for j, b in enumerate(biases):\n",
        "        print(f\"(w={w:.1f}, b={b:.1f})\")\n",
        "\n",
        "        # setup weights of model\n",
        "        model.w = w\n",
        "        model.b = b\n",
        "\n",
        "        sum_loss = 0\n",
        "        # for each example\n",
        "        for (x, y) in zip(x_train, y_train):\n",
        "            # compute the loss for this example\n",
        "            single_loss = model.loss(x, y)\n",
        "\n",
        "            # and add it to the sum\n",
        "            sum_loss += single_loss\n",
        "\n",
        "            # print out the values just to make sure everything is working correctly\n",
        "            y_hat = model.forward(x)\n",
        "            print(f\"    x: {x}, y: {y}, y_hat: {y_hat:.1f}, loss: {single_loss:.2f}\")\n",
        "\n",
        "        # print out the sum of individual losses\n",
        "        # I multiplied by 0.5 to be consistent with the equation earlier,\n",
        "        # but this is not necessary in practice as this is a constant\n",
        "        print(f\"    Loss = {(0.5 * sum_loss):.4f}\\n\")\n",
        "\n",
        "        # store the losses and the corresponding (w,b) for later use\n",
        "        losses.append((0.5*sum_loss, w, b))\n",
        "\n",
        "        # store the losses in a matrix form for visualisation later\n",
        "        loss_matrix[i,j] = 0.5 * sum_loss\n",
        "\n",
        "# find combination with minimum loss\n",
        "(min_loss, best_w, best_b) = min(losses, key=lambda x:x[0])\n",
        "print(\"BEST:\")\n",
        "print(f\"w={best_w}, b={best_b}, loss={min_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdvNc6WQOwhv"
      },
      "source": [
        "Plotting the loss values as a surface graph gives you a picture of the \"optimisation landscape\" for the parameter values. The loss is minimum at $w=2$, $b=1$ (it might be hard to see this clearly from the 3D diagram, but you can trust the numbers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVruAU86OwIA"
      },
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "# enable 3D\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        " # generate combinations of weights and biases\n",
        "(w_list, b_list) = np.meshgrid(weights, biases)\n",
        "\n",
        "# plot loss across weights and bias values\n",
        "surf = ax.plot_surface(w_list.T, b_list.T, loss_matrix,\n",
        "                       linewidth=0, antialiased=False)\n",
        "\n",
        "ax.set_xlabel('w')\n",
        "ax.set_ylabel('b')\n",
        "ax.set_zlabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nWYSrkBcWzs"
      },
      "source": [
        "### Optimisation with derivatives\n",
        "\n",
        "Obviously, the brute force search method above is really computationally expensive (very slow!), especially when there are more parameters and more values to search. A more efficient approach is to start from a random set of parameter values, and then try to \"move down\" the loss graph to the point with a minimum loss. So, in the plot above, you can start at some random $w$ and $b$, and try to move towards the minimum point.\n",
        "\n",
        "The problem is: from where you are, you actually do not know where the minimum point is. You only know that there *is* a minimum point. The question is: how do you know towards which direction you should move (and how far) so that you can get to the minimum point as fast as possible? The answer is downwards (obviously!) and where the slope is steepest.\n",
        "\n",
        "Fortunately, calculus saves us from guessing, and provides us a way to compute the direction of the slope at any point. This is called the *derivative* (or gradient). Intuitively, the derivative $\\frac{\\partial L}{\\partial w}$ tells us by how much $L$ changes when $w$ changes. Similarly, $\\frac{\\partial L}{\\partial b}$ tells us by how much $L$ changes when $b$ changes. So if we move in these directions, we hope that we can ultimately reach a minimum point.\n",
        "\n",
        "If you are well-versed with calculus, you can compute $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ by hand. Otherwise, you can get help from a derivative calculator (e.g. https://www.derivative-calculator.net)\n",
        "\n",
        "As presented in the lectures, the partial derivatives of the loss function with respect to $w$ and to $b$ are:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w} = \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x^{(i)}$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$\n",
        "\n",
        "Thus, for a *single* point, the partial derivatives are:\n",
        "\n",
        "$\\frac{\\partial L^{(i)}}{\\partial w} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x^{(i)}$\n",
        "\n",
        "$\\frac{\\partial L^{(i)}}{\\partial b} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$\n",
        "\n",
        "Now, complete the `gradient()` method for `SimpleLinearRegression` below to compute the partial derivatives wrt $w$ and $b$ at a given point. To make life easier, just compute and return both $\\frac{\\partial L^{(i)}}{\\partial w}$ and $\\frac{\\partial L^{(i)}}{\\partial b}$ at the same time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBXmI71Ewc9e"
      },
      "source": [
        "def gradient(self, x, y):\n",
        "    \"\"\" Compute partial derivatives wrt w and b\n",
        "\n",
        "    Args:\n",
        "        x (float): input instance\n",
        "        y (float): ground truth output\n",
        "\n",
        "    Returns:\n",
        "        tuple: (float, float)\n",
        "            - the first element will be dL/dw\n",
        "            - the second element will be dL/db\n",
        "    \"\"\"\n",
        "    # TODO: Complete this\n",
        "\n",
        "    return ????\n",
        "\n",
        "\n",
        "# A quick hack to bind this function as the SimpleLinearRegression.gradient() method\n",
        "SimpleLinearRegression.gradient = gradient\n",
        "\n",
        "\n",
        "## Quick test: This should return (6.0, 3.0)\n",
        "model = SimpleLinearRegression()\n",
        "model.w = 3\n",
        "model.b = 2\n",
        "x = 2.0\n",
        "y = 5.0\n",
        "(dLdw, dLdb) = model.gradient(x, y)\n",
        "print(dLdw) # should print 6.0\n",
        "print(dLdb) # should print 3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBw1pRVN-heL"
      },
      "source": [
        "So, in this example, $\\frac{\\partial L}{\\partial w}=6.0$ suggests that when $w$ increases by a very tiny amount, $L$ will increase by 6.0 times that amount.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCNctFEY0yiT"
      },
      "source": [
        "#### Gradient descent\n",
        "\n",
        "Now that we have our gradients, let us try to optimise the parameters $w$ and $b$ of our model to minimise the loss. We will use the gradient descent algorithm for this, as discussed in the lectures.\n",
        "\n",
        "You may reimplement the code provided in the lectures, replacing some of the code by reusing the `forward()`, `loss()` and `gradient()` methods of `SimpleLinearRegression` that you implemented earlier. This will help make your code more modular and readable, and help to improve your understanding of gradient descent at a more abstract level.\n",
        "\n",
        "You should be able to obtain $w \\approx 2$ and $b \\approx 1$ by the end of training if you have implemented everything correctly. Also experiment with the learning rate and the number of epochs and observe the effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLdqp4ESCpsR"
      },
      "source": [
        "model = SimpleLinearRegression()\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    error = 0.0\n",
        "    grad_w = 0.0\n",
        "    grad_b = 0.0\n",
        "    for (x, y) in zip(x_train, y_train):\n",
        "        ### TODO: Complete this\n",
        "        ### 1. Compute the gradients for w and b for this example\n",
        "        ???\n",
        "\n",
        "        ### 2. Add the gradients to grad_w and grad_b\n",
        "        grad_w = ???\n",
        "        grad_b = ???\n",
        "\n",
        "        ### 3. Add the \"local\" loss to the global error (Loss) for analysis\n",
        "        error = ???\n",
        "\n",
        "    # TODO: Update the weights using the (summed) gradients\n",
        "    model.w = ???\n",
        "    model.b = ???\n",
        "\n",
        "    print(f\"Epoch: {epoch}\\t w: {model.w:.2f}\\t b: {model.b:.2f}\\t L: {error:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad3Q827dMohe"
      },
      "source": [
        "### Predictions\n",
        "\n",
        "Now that your model is trained, you can use it to predict some unknown test instances. Complete the code below to predict the output of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTqEAz6GMoRH"
      },
      "source": [
        "y_predictions = np.zeros((len(y_test),))\n",
        "for (i, x) in enumerate(x_test):\n",
        "    # TODO: Complete this\n",
        "    y_predictions[i] = ????\n",
        "\n",
        "print(y_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2ANPLrkLVaY"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Finally, let us evaluate the linear regression model you developed. Unlike classification, we will need a different metric for regression. Recall from Lecture 3, a common evaluation metric for regression is the Mean Squared Error (MSE). We will use that for this tutorial.\n",
        "\n",
        "Complete the `mse()` function below to compute the MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fvD2vNXLx6f"
      },
      "source": [
        "def mse(y_gold, y_prediction):\n",
        "    \"\"\" Compute the MSE given the ground truth and predictions\n",
        "\n",
        "    Args:\n",
        "        y_gold (np.ndarray): the correct ground truth values of y\n",
        "        y_prediction (np.ndarray): the predicted values of y\n",
        "\n",
        "    Returns:\n",
        "        float : MSE\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(y_gold) == len(y_prediction)\n",
        "\n",
        "    # TODO: Complete this\n",
        "    return ????\n",
        "\n",
        "\n",
        "# Compute the MSE on model predictions on our toy test data\n",
        "# You should be able to obtain a very small MSE rate\n",
        "print(mse(y_test, y_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6emtY9qqwGSe"
      },
      "source": [
        "## Iris Dataset (Extra exercise)\n",
        "\n",
        "Here is an extra optional exercise for you: try to get your simple linear regression model working on a (slightly) larger and noisier dataset.\n",
        "\n",
        "For this, we will convert the Iris dataset to use as a regression task. More specifically, our task is to predict the *petal width* of a flower (`y`) given the *sepal length* as input (`x`). The code below will give you the dataset in this format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1yTkyHzwnUp"
      },
      "source": [
        "import os\n",
        "\n",
        "# Download iris data if it does not exist\n",
        "if not os.path.exists(\"iris.data\"):\n",
        "    !wget -O iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "\n",
        "def read_dataset_as_regression(filepath):\n",
        "    \"\"\" Read in the dataset from the specified filepath\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The filepath to the dataset file\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x, y), each being a numpy array.\n",
        "               - x is a numpy array with shape (N, ),\n",
        "                   where N is the number of instances\n",
        "               - y is a numpy array with shape (N, ), where each element is a\n",
        "                real-valued float, and N is the number of instances\n",
        "    \"\"\"\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for line in open(filepath):\n",
        "        if line.strip() != \"\": # handle empty rows in file\n",
        "            row = line.strip().split(\",\")\n",
        "            # extract columns 0 as x.\n",
        "            x.append(float(row[0]))\n",
        "\n",
        "            # extract column 3 as y\n",
        "            y.append(float(row[3]))\n",
        "\n",
        "    return (np.array(x), np.array(y))\n",
        "\n",
        "\n",
        "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
        "    \"\"\" Split dataset into training and test sets, according to the given\n",
        "        test set proportion.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Output label, numpy array with shape (N,)\n",
        "        test_proprotion (float): the desired proportion of test examples\n",
        "                                 (0.0-1.0)\n",
        "        random_generator (np.random.Generator): A random generator\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x_train, x_test, y_train, y_test)\n",
        "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
        "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
        "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
        "               - y_test (np.ndarray): Test labels, shape (N_test, )\n",
        "    \"\"\"\n",
        "\n",
        "    shuffled_indices = random_generator.permutation(len(x))\n",
        "    n_test = round(len(x) * test_proportion)\n",
        "    n_train = len(x) - n_test\n",
        "    x_train = x[shuffled_indices[:n_train]]\n",
        "    y_train = y[shuffled_indices[:n_train]]\n",
        "    x_test = x[shuffled_indices[n_train:]]\n",
        "    y_test = y[shuffled_indices[n_train:]]\n",
        "    return (x_train, x_test, y_train, y_test)\n",
        "\n",
        "\n",
        "(x, y) = read_dataset_as_regression(\"iris.data\")\n",
        "print(x.shape)  # (150,)\n",
        "print(y.shape)  # (150,)\n",
        "\n",
        "seed = 60012\n",
        "rg = default_rng(seed)\n",
        "x_train, x_test, y_train, y_test = split_dataset(x, y,\n",
        "                                                 test_proportion=0.2,\n",
        "                                                 random_generator=rg)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XM2yXt_0RMO"
      },
      "source": [
        "As usual, it's always a good idea to examine your data before you start:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTF3kSAV0ddn"
      },
      "source": [
        "plt.figure()\n",
        "plt.scatter(x_train, y_train, c=\"blue\", edgecolor='k')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYm-2dB41JYI"
      },
      "source": [
        "### Model\n",
        "\n",
        "You can copy your `SimpleLinearRegression` class from earlier, with the`forward()`, `loss()` and `gradient()` methods that you implemented earlier. This time you can put them all together in the same place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfdsAHaV1IuR"
      },
      "source": [
        "class SimpleLinearRegression:\n",
        "    def __init__(self, random_generator=default_rng()):\n",
        "        self.w = random_generator.standard_normal()\n",
        "        self.b = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        return ????\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        return ????\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        return ????"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6zXgIed3gTR"
      },
      "source": [
        "### Optimisation\n",
        "\n",
        "Again, you should not need to change much of your gradient descent implementation from earlier, so just copy it over. You may, however, need to tweak the learning rate and the number of epochs to obtain a reasonable output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Txe_vse3iIH"
      },
      "source": [
        "model = SimpleLinearRegression()\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    error = 0.0\n",
        "    grad_w = 0.0\n",
        "    grad_b = 0.0\n",
        "    for (x, y) in zip(x_train, y_train):\n",
        "        ### TODO: Complete this\n",
        "        ### 1. Compute the gradients for w and b for this example\n",
        "        ???\n",
        "\n",
        "        ### 2. Add the gradients to grad_w and grad_b\n",
        "        grad_w = ???\n",
        "        grad_b = ???\n",
        "\n",
        "        ### 3. Add the \"local\" loss to the global error (Loss) for analysis\n",
        "        error = ???\n",
        "\n",
        "    # TODO: Update the weights using the (summed) gradients\n",
        "    model.w = ???\n",
        "    model.b = ???\n",
        "\n",
        "    print(f\"Epoch: {epoch}\\t w: {model.w:.2f}\\t b: {model.b:.2f}\\t L: {error:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P7tQX-L31LB"
      },
      "source": [
        "### Visualising your trained model\n",
        "\n",
        "You can visualise your model by plotting the line on the graph.\n",
        "\n",
        "We will also plot the test instances to get a rough idea of how well we expect it to perform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUmFEg7s33Ve"
      },
      "source": [
        "# Plot training instances\n",
        "plt.figure()\n",
        "plt.scatter(x_train, y_train, c=\"blue\", edgecolor='k')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "\n",
        "# Draw the line representing the model\n",
        "xmin = x_train.min()\n",
        "ymin = model.forward(xmin)\n",
        "xmax = x_train.max()\n",
        "ymax = model.forward(xmax)\n",
        "plt.plot([xmin, xmax], [ymin, ymax], 'r-')\n",
        "\n",
        "# Plot test instances\n",
        "plt.scatter(x_test, y_test, c=\"red\", edgecolor='k')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY37EHKe5JZT"
      },
      "source": [
        "### Predictions and evaluation\n",
        "\n",
        "Finally, predict the test instances given the model.\n",
        "\n",
        "Then evaluate the model with MSE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHx0gaAr5OfN"
      },
      "source": [
        "y_predictions = np.zeros((len(y_test),))\n",
        "for (i, x) in enumerate(x_test):\n",
        "    y_predictions[i] = ????\n",
        "\n",
        "print(y_predictions)\n",
        "print(y_test)\n",
        "\n",
        "print(mse(y_test, y_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMajcyH7Vv5"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Hopefully you have managed to deepen your understanding about linear regression by implementing the model, loss function, and the gradient descent algorithm, and putting everything together for training and testing.\n",
        "\n",
        "In the next lab exercise, we will delve a bit deeper at implementation level, and try to extend your model to handle more than one input variable. We will also start making your code a bit more efficient with vectorised implementations so that you can perform computations on multiple training instances simultaneously. This will hopefully help you get started on implementing Neural Networks (which we will unfortunately not cover in these lab exercises as it is part of your second coursework).  \n",
        "\n"
      ]
    }
  ]
}